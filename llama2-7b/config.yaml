data_store:
  hf_dataset_name: wikicorpus
  hf_dataset_config_name: raw_en
  local_path: data/wikicorpus_llama2_7B_tokenized_4k
  s3_prefix: wikicorpus_llama2_7B_tokenized_4k
  block_size: 4096
tokenizer_store:
  local_path: tokenizer.model
  s3_prefix: llama2_7b/tokenizer
model_store:
  local_weights_path: model
  local_checkpoints_path: model/checkpoints
  s3_prefix: llama2_7b/model
  s3_checkpoints_key: checkpoints
  s3_experiments_key: experiments
training:
  tensor_parallelism_degree: 8
  use_mix_precision: true
  use_zero_1: true
  global_batch_size: 1024
  micro_batch_size: 1
  learning_rate: 0.0003
  sequence_length: 4096
  do_pre_compilation: true
  pre_compilation_steps: 1
  warmup_steps: 3
  steps_this_run: 100
  total_steps: 100
  logging_interval: 1
  checkpoint_frequency: 50
  metrics_file: metrics.json
model_architecture:
  architectures:
  - LlamaForCausalLM
  bos_token_id: 1
  eos_token_id: 2
  hidden_act: silu
  hidden_size: 4096
  initializer_range: 0.02
  intermediate_size: 11008
  max_position_embeddings: 2048
  model_type: llama
  num_attention_heads: 32
  num_hidden_layers: 32
  num_key_value_heads: 32
  pad_token_id: 0
  pretraining_tp: 1
  rms_norm_eps: 1.0e-05
  rope_scaling: null
  tie_word_embeddings: false
  torch_dtype: float16
  transformers_version: 4.31.0
  use_cache: true
  vocab_size: 32000
  sequence_parallel_enabled: false
  selective_checkpoint_enabled: false
  move_model_to_device: true
